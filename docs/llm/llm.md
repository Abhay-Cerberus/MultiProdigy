# ğŸ§  MultiProdigy â€” Local LLM Client Framework (Offline + Simulated)
A modular Python framework to run and test multiple LLM clients â€” like OpenAI, Gemini, HuggingFace (Offline mode), and Ollama â€” all in one place. Ideal for environments with no API keys or internet required (offline-friendly). Supports full simulation and real LLM output using local models.

## ğŸ“ Folder Structure
```bash
MultiProdigy/
â”œâ”€â”€ demo/
â”‚   â””â”€â”€ llm_demo.py               # Main demo runner for all LLM clients
â”œâ”€â”€ llm/
â”‚   â”œâ”€â”€ clients.py                # OpenAI, Gemini, Ollama clients (simulated/real)
â”‚   â””â”€â”€ huggingface_client.py     # HuggingFace client using offline local models
â”œâ”€â”€ requirements.txt              # Required libraries (transformers, torch, etc.)
â””â”€â”€ README.md                     # You're here!
```
---------------
--------------

## ğŸ’¡ How It Works

- ğŸ”· OpenAI & Gemini (Simulated)

These are simulated LLMs without internet or API keys.

Return fake/test responses like: "Simulated response from OpenAI for: <your input>".

---------

- ğŸŸ¡ HuggingFace (Offline)

Works without internet or token using transformers library.

Loads models like gpt2, distilgpt2 locally.

Uses pipeline("text-generation", model=model_name) to generate text.

-------------

- ğŸŸ¢ Ollama (Real Local LLM)

Runs real Ollama models like tinyllama, mistral, llama3.

Requires ollama installed and model pulled via CLI:
```bash
ollama pull tinyllama
```

ğŸ§ª Demo File Overview: demo/llm_demo.py
```python
# llm_demo.py
async def demo_openai():           # Simulated response
async def demo_gemini():           # Simulated response
async def demo_huggingface():      # Runs offline HuggingFace model (gpt2 etc.)
async def demo_ollama():           # Runs real Ollama response using local model
```

Runs like this:

```sql
ğŸŒ Running all LLM Clients Demo...

ğŸ”· OpenAI Response: Simulated response from OpenAI for: Hello from OpenAI
ğŸŸ£ Gemini Response: Simulated response from Gemini for: Hello from Gemini
ğŸŸ¡ HuggingFace (Offline) Response: <Actual generated text from local model>
ğŸŸ¢ Ollama Response: <Real AI output from local Ollama model>
âœ… All clients ran successfully!
```

--------------------

- ğŸ§  LLM Clients


  - llm/clients.py

      Defines each LLM client interface:

      OpenAIClient â€“ Simulated, returns test strings

      GeminiClient â€“ Simulated, returns test strings

      OllamaClient â€“ Uses subprocess to call ollama run command

  - llm/huggingface_client.py

    Offline LLM using Hugging Face transformers.

    Loads and runs text-generation locally like this:

    ```python
    from transformers import pipeline
    self.generator = pipeline("text-generation", model="gpt2")
    ```

--------------
-----------

â–¶ï¸ How to Run
âœ… Step-by-step
Clone or set up the folder

Install dependencies:

```bash
pip install -r requirements.txt
```

install Ollama if you havent:
```bash
# For Windows or Mac
https://ollama.com/download
```

Make sure Ollama is installed and a model is pulled:

```bash
ollama pull tinyllama
```

Run the demo:

```bash
PYTHONPATH=. python demo/llm_demo.py
```
You will see:

```yaml
ğŸ”· OpenAI Response: ...
ğŸŸ£ Gemini Response: ...
ğŸŸ¡ HuggingFace (Offline) Response: ...
ğŸŸ¢ Ollama Response: ...
```

âœ… What's Offline and What's Not?

Client	Offline?	Notes

OpenAIClient	âœ… Yes	Simulated (no real API call)

GeminiClient	âœ… Yes	Simulated (no real API call)

HuggingFace	  âœ… Yes	Requires downloaded models only

Ollama	      âœ… Yes	Runs real local models via CLI
